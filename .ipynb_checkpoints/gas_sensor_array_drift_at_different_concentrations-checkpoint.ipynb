{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac73ebf-7ee4-4363-acd0-bdaa1fd89924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 270, 'name': 'Gas Sensor Array Drift at Different Concentrations', 'repository_url': 'https://archive.ics.uci.edu/dataset/270/gas+sensor+array+drift+dataset+at+different+concentrations', 'data_url': 'https://archive.ics.uci.edu/static/public/270/data.csv', 'abstract': 'This archive contains 13910 measurements from 16 chemical sensors exposed to 6 different gases at various concentration levels.', 'area': 'Computer Science', 'tasks': ['Classification', 'Regression', 'Clustering', 'Causa'], 'characteristics': ['Multivariate', 'Time-Series'], 'num_instances': 13910, 'num_features': 128, 'feature_types': ['Real'], 'demographics': [], 'target_col': ['class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2012, 'last_updated': 'Mon Apr 08 2024', 'dataset_doi': '10.24432/C5MK6M', 'creators': ['Alexander Vergara'], 'intro_paper': {'title': 'On the calibration of sensor arrays for pattern recognition using the minimal number of experiments', 'authors': 'Irene Rodríguez-Luján, J. Fonollosa, A. Vergara, M. Homer, R. Huerta', 'published_in': 'Chemometrics and Intelligent Laboratory Systems 130', 'year': 2014, 'url': 'https://www.semanticscholar.org/paper/On-the-calibration-of-sensor-arrays-for-pattern-the-Rodr%C3%ADguez-Luj%C3%A1n-Fonollosa/d963fe1234694fba4048b0951f29dbe9ab7f8923', 'doi': None}, 'additional_info': {'summary': \"This data set contains 13,910 measurements from 16 chemical sensors exposed to 6 gases at different concentration levels. This dataset is an extension of the Gas Sensor Array Drift Dataset (http://archive.ics.uci.edu/ml/datasets/Gas+Sensor+Array+Drift+Dataset), providing now the information about the concentration level at which the sensors were exposed for each measurement. The primary purpose of making this dataset freely accessible on-line is to provide an extensive dataset to the sensor and artificial intelligence research communities to develop and test strategies to solve a wide variety of tasks, including sensor drift, classification, regression, among others. \\r\\n\\r\\nThe dataset can be used exclusively for research purposes. Commercial purposes are fully excluded. Citation of both Vergara et al. 'Chemical gas sensor drift compensation using classifier ensembles' and Rodriguez-Lujan et al. â€œOn the calibration of sensor arrays for pattern recognition using the minimal number of experimentsâ€\\x9d is required (see below).\\r\\n\\r\\nThe dataset was gathered during the period of January 2008 to February 2011 (36 months) in a gas delivery platform facility situated at the ChemoSignals Laboratory in the BioCircuits Institute, University of California San Diego. The measurement system platform provides versatility for obtaining the desired concentrations of the chemical substances of interest with high accuracy and in a highly reproducible manner, minimizing thereby the common mistakes caused by human intervention and making it possible to exclusively concentrate on the chemical sensors. See reference 1 for more details on the experimental setup. \\r\\n\\r\\nThe resulting dataset comprises recordings from six distinct pure gaseous substances, namely Ammonia, Acetaldehyde, Acetone, Ethylene, Ethanol, and Toluene, dosed at a wide variety of concentration levels in the intervals (50,1000), (5,500), (12,1000), (10,300), (10,600), and (10,100) ppmv, respectively.\", 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'The responses of the said sensors are read in the form of the resistance across the active layer of each sensor; hence, each measurement produced a 16-channel time series, each  represented by an aggregate of features reflecting the dynamic processes occurring at the sensor surface in reaction to the chemical substance being evaluated. In particular, two distinct types of features were considered in the creation of this dataset: (i) the so-called steady-state feature (DR), defined as the maximal resistance change with respect to the baseline and its DR normalized version (DR divided by the acquired value when the chemical vapor is present in the test chamber). And (ii), an aggregate of features reflecting the sensor dynamics of the increasing/decaying transient portion of the sensor response during the entire measurement. This aggregate of features is a transformation, borrowed from the field of econometrics and originally introduced to the chemo-sensing community by Muezzinoglu et al. (2009), that converts the transient portion of the sensor response into a real scalar by estimating the maximum/minimum value y[k] for the rising/decaying portion of the exponential moving average of the sensor response:\\r\\n\\r\\ny[k] = (1-Alfa) y[k-1]+Alfa(R[k]-R[k-1])\\r\\n\\r\\nwhere R[k] is the sensor resistance measured at time k and Alfa is a scalar smoothing parameter between 0 and 1.\\r\\n\\r\\nIn particular, three different values for Alfa=0.1, 0.01, 0.001 were set to obtain three different feature values from the rising portion of the sensor response and three additional features with the same Alfa values for the decaying portion of the sensor response, covering thus the entire sensor response dynamics. \\r\\n\\r\\nThus, each feature vector contains the 8 features extracted from each particular sensor, resulting in a 128-dimensional feature vector (8 features x 16 sensors) containing all the features and organized as follows:\\r\\nDR_1, |DR|_1, EMAi0.001_1, EMAi0.01_1, EMAi0.1_1, EMAd0.001_1, EMAd0.01_1, EMAd0.1_1, DR_2, |DR|_2, EMAi0.001_2, EMAi0.01_2, EMAi0.1_2, EMAd0.001_2, EMAd0.01_2, EMAd0.1_2,..., DR_16, |DR|_16, EMAi0.001_16, EMAi0.01_16, EMAi0.1_16, EMAd0.001_16, EMAd0.01_16, EMAd0.1_16\\r\\nwhere: DR_j and |DR|_j are the R and the normalized R features, respectively. EMAi0.001_j, EMAi0.01_j, and EMAi0.1_j, are the emaR of the rising transient portion of the sensor response for Alfa 0.001, 0.01, and 0.1, respectively. EMAd0.001_j, EMAd0.01_j, and EMAd0.1_j, are emaR of the decaying transient portion of the sensor response for Alfa 0.001, 0.01, and 0.1, respectively. The index j=1â€¦16 represents the number of the sensor, forming thus the 128-dimensional feature vector. \\r\\n\\r\\nFor processing purposes, the dataset is organized into ten batches, each containing the number of measurements per class and month indicated in the tables below. This reorganization of data was done to ensure having a sufficient and as uniformly distributed as possible number of experiments in each batch. \\r\\n\\r\\nBatch ID\\tMonth IDs \\r\\nBatch 1\\t Months 1 and 2 \\r\\nBatch 2\\t Months 3, 4, 8, 9 and 10 \\r\\nBatch 3\\t Months 11, 12, and 13 \\r\\nBatch 4\\t Months 14 and 15 \\r\\nBatch 5\\t Month 16 \\r\\nBatch 6\\t Months 17, 18, 19, and 20 \\r\\nBatch 7\\t Month 21 \\r\\nBatch 8\\t Months 22 and 23 \\r\\nBatch 9\\t Months 24 and 30 \\r\\nBatch 10 Month 36 \\r\\n\\r\\nBatch ID: Ethanol, Ethylene, Ammonia, Acetaldehyde, Acetone, Toluene\\r\\nBatch 1: 83, 30, 70, 98, 90, 74\\r\\nBatch 2: 100, 109, 532, 334, 164, 5\\r\\nBatch 3: 216, 240, 275, 490, 365, 0\\r\\nBatch 4: 12, 30, 12, 43, 64, 0\\r\\nBatch 5: 20, 46, 63, 40, 28, 0\\r\\nBatch 6: 110, 29, 606, 574, 514, 467\\r\\nBatch 7: 360, 744, 630, 662, 649, 568\\r\\nBatch 8: 40, 33, 143, 30, 30, 18\\r\\nBatch 9: 100, 75, 78, 55, 61, 101\\r\\nBatch 10: 600, 600, 600, 600, 600, 600 \\r\\n\\r\\n\\r\\nThe dataset is organized in files, each representing a different batch. Within the files, each line represents a measurement. The first character (1-6) codes the analyte, followed by the concentration level:\\r\\n\\r\\n1: Ethanol; 2: Ethylene; 3: Ammonia; 4: Acetaldehyde; 5: Acetone; 6: Toluene\\r\\n\\r\\nThe data format follows the same coding style as in libsvm format x:v, where x stands for the feature number and v for the actual value of the feature. For example, in \\r\\n1;10.000000 1:15596.162100 2:1.868245 3:2.371604 4:2.803678 5:7.512213 â€¦ 128:-2.654529 \\r\\n\\r\\nThe number 1 stands for the class number (in this case Ethanol), the gas concentration level was 10ppmv, and the remaining 128 columns list the actual feature values for each measurement recording organized as described above. ', 'citation': None}}\n",
      "           name     role         type demographic description units  \\\n",
      "0         class   Target  Categorical        None        None  None   \n",
      "1      Feature1  Feature  Categorical        None        None  None   \n",
      "2      Feature2  Feature  Categorical        None        None  None   \n",
      "3      Feature3  Feature  Categorical        None        None  None   \n",
      "4      Feature4  Feature  Categorical        None        None  None   \n",
      "..          ...      ...          ...         ...         ...   ...   \n",
      "124  Feature124  Feature  Categorical        None        None  None   \n",
      "125  Feature125  Feature  Categorical        None        None  None   \n",
      "126  Feature126  Feature  Categorical        None        None  None   \n",
      "127  Feature127  Feature  Categorical        None        None  None   \n",
      "128  Feature128  Feature  Categorical        None        None  None   \n",
      "\n",
      "    missing_values  \n",
      "0               no  \n",
      "1               no  \n",
      "2               no  \n",
      "3               no  \n",
      "4               no  \n",
      "..             ...  \n",
      "124             no  \n",
      "125             no  \n",
      "126             no  \n",
      "127             no  \n",
      "128             no  \n",
      "\n",
      "[129 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "gas_sensor_array_drift_at_different_concentrations = fetch_ucirepo(id=270) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = gas_sensor_array_drift_at_different_concentrations.data.features \n",
    "y = gas_sensor_array_drift_at_different_concentrations.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(gas_sensor_array_drift_at_different_concentrations.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(gas_sensor_array_drift_at_different_concentrations.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "735c7952-f09f-4ee5-b331-37ea94ab1458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature1      float64\n",
      "Feature2      float64\n",
      "Feature3      float64\n",
      "Feature4      float64\n",
      "Feature5      float64\n",
      "               ...   \n",
      "Feature123    float64\n",
      "Feature124    float64\n",
      "Feature125    float64\n",
      "Feature126    float64\n",
      "Feature127    float64\n",
      "Length: 127, dtype: object\n",
      "Feature1      float64\n",
      "Feature2      float64\n",
      "Feature3      float64\n",
      "Feature4      float64\n",
      "Feature5      float64\n",
      "               ...   \n",
      "Feature123    float64\n",
      "Feature124    float64\n",
      "Feature125    float64\n",
      "Feature126    float64\n",
      "Feature127    float64\n",
      "Length: 127, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# X 데이터프레임의 각 열의 데이터 타입 확인\n",
    "print(X.dtypes)\n",
    "\n",
    "# 문자열이 포함된 열 확인\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        print(f\"Column '{column}' contains string data: {X[column].unique()[:5]}\")\n",
    "\n",
    "# 문자열이 포함된 열을 숫자로 변환하거나, 필요시 제거\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        try:\n",
    "            # 예: 문자열에서 숫자를 추출하는 방식\n",
    "            X[column] = X[column].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert column '{column}' to float, dropping column.\")\n",
    "            X = X.drop(columns=[column])\n",
    "\n",
    "# 최종 확인\n",
    "print(X.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f2172ad-15d8-4273-8de9-e32199447feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자열 처리 개선 (예시: LabelEncoder)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for column in X.columns:\n",
    "    if X[column].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        X[column] = le.fit_transform(X[column])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecc74864-fcf4-4489-bf1d-d6bef6857039",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nan_cols = X.columns[X.isna().sum() == len(X)]\n",
    "X = X.drop(nan_cols, axis=1)\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# 시퀀스 생성 함수 정의\n",
    "sequence_length = 10\n",
    "\n",
    "def create_sequences(X, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(X.shape[0] - sequence_length):\n",
    "        sequence = X[i:i + sequence_length]\n",
    "        sequences.append(sequence)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# 시퀀스 생성\n",
    "X_seq = create_sequences(X_scaled, sequence_length)\n",
    "y_seq = y[sequence_length:]\n",
    "\n",
    "# 훈련, 검증, 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "if y_train.ndim == 1:\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8e7e639-cac3-4a30-8d01-5e96c3a3d6b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot use a string pattern on a bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected label format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# y_train과 y_test 배열에 함수 적용\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m y_train_cleaned \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([extract_numeric_label(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m y_train])\n\u001b[0;32m     19\u001b[0m y_test_cleaned \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([extract_numeric_label(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m y_test])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 라벨 인코딩 및 원-핫 인코딩 (필요시)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 18\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected label format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# y_train과 y_test 배열에 함수 적용\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m y_train_cleaned \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mextract_numeric_label\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m y_train])\n\u001b[0;32m     19\u001b[0m y_test_cleaned \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([extract_numeric_label(label) \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m y_test])\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# 라벨 인코딩 및 원-핫 인코딩 (필요시)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[27], line 11\u001b[0m, in \u001b[0;36mextract_numeric_label\u001b[1;34m(label)\u001b[0m\n\u001b[0;32m      8\u001b[0m     label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 콜론(:) 앞의 숫자를 추출\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m match \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m(\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+):\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\re.py:191\u001b[0m, in \u001b[0;36mmatch\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmatch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    189\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern at the start of the string, returning\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot use a string pattern on a bytes-like object"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# 숫자 라벨을 추출하는 함수 정의\n",
    "def extract_numeric_label(label):\n",
    "    # 바이트형 데이터라면 문자열로 디코딩\n",
    "    if isinstance(label, bytes):\n",
    "        label = label.decode('utf-8')\n",
    "    \n",
    "    # 콜론(:) 앞의 숫자를 추출\n",
    "    match = re.match(r\"(\\d+):\", label)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected label format: {label}\")\n",
    "\n",
    "# y_train과 y_test 배열에 함수 적용\n",
    "y_train_cleaned = np.array([extract_numeric_label(label) for label in y_train])\n",
    "y_test_cleaned = np.array([extract_numeric_label(label) for label in y_test])\n",
    "\n",
    "# 라벨 인코딩 및 원-핫 인코딩 (필요시)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train_cleaned)\n",
    "y_test_encoded = label_encoder.transform(y_test_cleaned)\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train_encoded)\n",
    "y_test_one_hot = to_categorical(y_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ef7c045-39c4-437c-b6e3-c164f74a5f24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '1:25902.322200'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[0;32m     12\u001b[0m y_train_encoded \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(y_train)\n\u001b[1;32m---> 13\u001b[0m y_test_encoded \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# One-hot encode if needed\u001b[39;00m\n\u001b[0;32m     16\u001b[0m y_train_one_hot \u001b[38;5;241m=\u001b[39m to_categorical(y_train_encoded)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:132\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Transform labels to normalized encoding.\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \n\u001b[0;32m    121\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Labels as normalized encodings.\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 132\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcolumn_or_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclasses_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# transform of empty array is empty array\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:1219\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ravel column or 1d numpy array, else raises an error.\u001b[39;00m\n\u001b[0;32m   1194\u001b[0m \n\u001b[0;32m   1195\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;124;03m    If `y` is not a 1D array or a 2D array with a single row or column.\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1218\u001b[0m xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(y)\n\u001b[1;32m-> 1219\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1228\u001b[0m shape \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\validation.py:913\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39misdtype(array\u001b[38;5;241m.\u001b[39mdtype, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreal floating\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplex floating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    906\u001b[0m         _assert_all_finite(\n\u001b[0;32m    907\u001b[0m             array,\n\u001b[0;32m    908\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    912\u001b[0m         )\n\u001b[1;32m--> 913\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    915\u001b[0m     array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:245\u001b[0m, in \u001b[0;36m_NumPyAPIWrapper.astype\u001b[1;34m(self, x, dtype, copy, casting)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mastype\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, dtype, \u001b[38;5;241m*\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;66;03m# astype is not defined in the top level NumPy namespace\u001b[39;00m\n\u001b[1;32m--> 245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: '1:25902.322200'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, GRU, SimpleRNN, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.metrics import MeanSquaredError\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# LSTM 모델 정의\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=RMSprop(), loss='mse', metrics=[MeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "# GRU 모델 정의\n",
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(50, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=RMSprop(), loss='mse', metrics=[MeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "# SimpleRNN(tanh) 모델 정의\n",
    "def build_rnn_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(50, activation='relu', input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=RMSprop(), loss='mse', metrics=[MeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "# 조기 종료 설정\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# 모델 훈련\n",
    "lstm_model = build_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "gru_model = build_gru_model((X_train.shape[1], X_train.shape[2]))\n",
    "rnn_model = build_rnn_model((X_train.shape[1], X_train.shape[2]))\n",
    "\n",
    "lstm_history = lstm_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1, callbacks=[early_stopping])\n",
    "gru_history = gru_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1, callbacks=[early_stopping])\n",
    "rnn_history = rnn_model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da7991-37fd-4913-ae0a-50ecab27d834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 학습 및 검증 손실 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Epochs 기준 그래프\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(rnn_history.history['loss'], label='tanh train', color='blue')\n",
    "plt.plot(rnn_history.history['val_loss'], label='tanh valid', color='blue', linestyle='--')\n",
    "plt.plot(gru_history.history['loss'], label='GRU train', color='green')\n",
    "plt.plot(gru_history.history['val_loss'], label='GRU valid', color='green', linestyle='--')\n",
    "plt.plot(lstm_history.history['loss'], label='LSTM train', color='purple')\n",
    "plt.plot(lstm_history.history['val_loss'], label='LSTM valid', color='purple', linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.title('Per epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 시간 기준 그래프\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(np.cumsum(rnn_history.history['loss']), label='tanh train', color='blue')\n",
    "plt.plot(np.cumsum(rnn_history.history['val_loss']), label='tanh valid', color='blue', linestyle='--')\n",
    "plt.plot(np.cumsum(gru_history.history['loss']), label='GRU train', color='green')\n",
    "plt.plot(np.cumsum(gru_history.history['val_loss']), label='GRU valid', color='green', linestyle='--')\n",
    "plt.plot(np.cumsum(lstm_history.history['loss']), label='LSTM train', color='purple')\n",
    "plt.plot(np.cumsum(lstm_history.history['val_loss']), label='LSTM valid', color='purple', linestyle='--')\n",
    "plt.yscale('log')\n",
    "plt.title('Wall Clock Time (seconds)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Cumulative Loss')\n",
    "plt.legend()\n",
    "\n",
    "# 두 그래프를 합친 제목 추가\n",
    "plt.suptitle('Gas Sensor Array Drift at Different Concentrations', fontsize=16)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # suptitle과 그래프가 겹치지 않도록 조정\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc44050-d910-4112-af7c-d7cafb102dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
